{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from dlcliche.image import *\n",
    "\n",
    "from fastai.vision import *\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn\n",
    "import PIL\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.60'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fastai\n",
    "fastai.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#田久保が作成したファイル\n",
    "import get_embedding\n",
    "import imscatter\n",
    "import grad_cam\n",
    "import body_feature_model\n",
    "#クラスタリング\n",
    "import image_clustering_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FILE_NAME = \"image/\"\n",
    "MODEL_FILE = \"model/\"\n",
    "\n",
    "def prepare_full_MNIST_databunch(data_folder, tfms):\n",
    "    \"\"\"\n",
    "    Prepare dataset as images under:\n",
    "        data_folder/images/('train' or 'valid')/(class)\n",
    "    where filenames are:\n",
    "        img(class)_(count index).png\n",
    "    \"\"\"\n",
    "    train_ds = datasets.MNIST(data_folder, train=True, download=True,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                          ]))\n",
    "    valid_ds = datasets.MNIST(data_folder, train=False,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                              ]))\n",
    "    test_ds = datasets.MNIST(data_folder, train=False,\n",
    "                              transform=transforms.Compose([\n",
    "                                  transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                              ]))\n",
    "\n",
    "    def have_already_been_done():\n",
    "        return (data_folder/'images').is_dir()\n",
    "    def build_images_folder(data_root, X, labels, dest_folder):\n",
    "        images = data_folder/'images'\n",
    "        for i, (x, y) in tqdm.tqdm(enumerate(zip(X, labels))):\n",
    "            folder = images/dest_folder/f'{y}'\n",
    "            ensure_folder(folder)\n",
    "            x = x.numpy()\n",
    "            image = np.stack([x for ch in range(3)], axis=-1)\n",
    "            PIL.Image.fromarray(image).save(folder/f'img{y}_{i:06d}.jpg')\n",
    "\n",
    "    if not have_already_been_done():\n",
    "        build_images_folder(data_root=DATA, X=train_ds.train_data,\n",
    "                            labels=train_ds.train_labels, dest_folder='train')\n",
    "        build_images_folder(data_root=DATA, X=valid_ds.test_data, \n",
    "                            labels=valid_ds.test_labels, dest_folder='valid')\n",
    "        build_images_folder(data_root=DATA, X=test_ds.test_data, \n",
    "                            labels=test_ds.test_labels, dest_folder='test')\n",
    "\n",
    "    return ImageDataBunch.from_folder(data_folder/'images', ds_tfms=tfms,bs=16)\n",
    "\n",
    "\n",
    "def body_feature_model(model,takubo=0):\n",
    "    \"\"\"\n",
    "    Returns a model that output flattened features directly from CNN body.\n",
    "    \"\"\"\n",
    "    if takubo == 0 :\n",
    "        try:\n",
    "            body, head = list(model.org_model.children()) # For XXNet defined in this notebook\n",
    "        except:\n",
    "            body, head = list(model.children()) # For original pytorch model\n",
    "    \n",
    "    return nn.Sequential(body, head[:-1])\n",
    "\n",
    "\n",
    "def get_embeddings(embedding_model, data_loader, label_catcher=None, return_y=False):\n",
    "    \"\"\"\n",
    "    Calculate embeddings for all samples in a data_loader.\n",
    "    \n",
    "    Args:\n",
    "        label_catcher: LearnerCallback for keeping last batch labels.\n",
    "        return_y: Also returns labels, for working with training set.\n",
    "    \"\"\"\n",
    "    embs, ys = [], []\n",
    "    xs = []\n",
    "    for X, y in data_loader:\n",
    "        # For each batch (X, y),\n",
    "        #   Set labels (y) if label_catcher's there.\n",
    "        if label_catcher:\n",
    "            label_catcher.on_batch_begin(X, y, train=False)\n",
    "        #   Get embeddings for this batch, store in embs.\n",
    "        with torch.no_grad():\n",
    "            # Note that model's output is not softmax'ed.\n",
    "            out = embedding_model(X).cpu().detach().numpy()\n",
    "            out = out.reshape((len(out), -1))\n",
    "            embs.append(out)\n",
    "        \n",
    "        for i in X:\n",
    "            xs.append(i.data.to(\"cuda\").reshape(1, 3, 224, 224))\n",
    "        y = y.detach().cpu().numpy()\n",
    "        ys.append(y)\n",
    "    # Putting all embeddings in shape (number of samples, length of one sample embeddings)\n",
    "    embs = np.concatenate(embs) # Maybe in (10000, 10)\n",
    "    ys = np.concatenate(ys,0)\n",
    "    #xs = np.concatenate(xs) \n",
    "    if return_y:\n",
    "        return embs, ys, xs\n",
    "    return embs\n",
    "\n",
    "def get_embeddings_one(embedding_model, image, label, return_y=False):\n",
    "    \"\"\"\n",
    "    Calculate embeddings for all samples in a data_loader.\n",
    "    \n",
    "    Args:\n",
    "        label_catcher: LearnerCallback for keeping last batch labels.\n",
    "        return_y: Also returns labels, for working with training set.\n",
    "    \"\"\"\n",
    "    embs = []\n",
    "    with torch.no_grad():\n",
    "            # Note that model's output is not softmax'ed.\n",
    "        out = embedding_model(image).cpu().detach().numpy()\n",
    "        out = out.reshape((len(out), -1))\n",
    "        embs.append(out)        \n",
    "    # Putting all embeddings in shape (number of samples, length of one sample embeddings)\n",
    "    embs = np.concatenate(embs) # Maybe in (10000, 10)\n",
    "    #xs = np.concatenate(xs) \n",
    "    if return_y:\n",
    "        return embs, label, image\n",
    "    return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "#%matplotlib notebook\n",
    "\n",
    "def show_2D_tSNE(latent_vecs, target, title='t-SNE viz'):\n",
    "    latent_vecs = latent_vecs\n",
    "    latent_vecs_reduced = TSNE(n_components=2, random_state=0).fit_transform(latent_vecs)\n",
    "    plt.scatter(latent_vecs_reduced[:, 0], latent_vecs_reduced[:, 1],\n",
    "                c=target, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.savefig(FILE_NAME+title+\".jpg\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_3D_tSNE(latent_vecs, target, title='3D t-SNE viz'):\n",
    "    latent_vecs = latent_vecs\n",
    "    tsne = TSNE(n_components=3, random_state=0).fit_transform(latent_vecs)\n",
    "    fig = plt.figure(figsize=(13,10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    scatter = ax.scatter3D(tsne[:, 0], tsne[:, 1], tsne[:, 2], c=target, cmap='jet')\n",
    "    ax.set_title(title)\n",
    "    plt.colorbar(scatter)\n",
    "    plt.savefig(FILE_NAME+title+\".jpg\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_as_PCA(latent_vecs, target, title='PCA viz'):\n",
    "    latent_vecs = latent_vecs\n",
    "    #latent_vecs_reduced = TSNE(n_components=2, random_state=0).fit_transform(latent_vecs)\n",
    "    latent_vecs_reduced = PCA(n_components=2).fit_transform(latent_vecs)\n",
    "    plt.scatter(latent_vecs_reduced[:, 0], latent_vecs_reduced[:, 1],\n",
    "                c=target, cmap='jet')\n",
    "    plt.colorbar()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import cv2\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\"\"\"\n",
    "t-sneで次元削除をしてそのグラフに画像をはる関数\n",
    "image_list　画像ファイル名\n",
    "latent_vecs　特徴抽出した画像の値\n",
    "title　グラフ画像の保存名\n",
    "\"\"\"\n",
    "\n",
    "path_l=[]\n",
    "for i in [4,0,6,2,1,3,5]:\n",
    "    \n",
    "    path = pathlib.Path('/home/cvmlab/Desktop/fastai/data/images/valid/'+str(i)+\"/\").glob('*.jpg')\n",
    "    for p in path:\n",
    "        path_l.append(p)\n",
    "\n",
    "def imscatter(image_list,latent_vecs, title, ax=None, zoom=1):\n",
    "    \n",
    "    latent_vecs = latent_vecs\n",
    "    kills_reduced = TSNE(n_components=2, random_state=0).fit_transform(latent_vecs)\n",
    "    #tsne = TSNE(n_jobs=4, perplexity=20) # 20が一番いい感じでした\n",
    "    #kills_reduced = tsne.fit_transform(latent_vecs)\n",
    "    x,y= kills_reduced[:,0], kills_reduced[:,1]\n",
    "    \n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    im_list = [OffsetImage(plt.imread(str(p)), zoom=zoom) for p in image_list]\n",
    "    x, y = np.atleast_1d(x, y)\n",
    "    artists = []\n",
    "    for x0, y0, im in zip(x, y, im_list):\n",
    "        ab = AnnotationBbox(im, (x0, y0), xycoords='data', frameon=False)\n",
    "        artists.append(ax.add_artist(ab))\n",
    "    ax.update_datalim(np.column_stack([x, y]))\n",
    "    ax.autoscale()\n",
    "    return artists\n",
    "\n",
    "    # perplexity: 20\n",
    "    fig, ax = plt.subplots(figsize=(30,30))\n",
    "    \n",
    "    plt.savefig(FILE_NAME+title+\".jpg\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelList (202 items)\n",
       "x: ImageList\n",
       "Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224),Image (3, 224, 224)\n",
       "y: CategoryList\n",
       "4,4,4,4,4\n",
       "Path: data/images"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MNISTがない場合は新しくつくってしまうので注意\n",
    "DATA = Path('data')\n",
    "data = prepare_full_MNIST_databunch(DATA, get_transforms(do_flip=False))\n",
    "#image_path=[4,0,6,2,1,3,5]\n",
    "image_path=[4,0,6,2,1,3,5]\n",
    "data.valid_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from metrics import *\n",
    "except:\n",
    "    ! wget https://raw.githubusercontent.com/ronghuaiyang/arcface-pytorch/master/models/metrics.py\n",
    "    from metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ArcFaceで学習を行うファイル\n",
    "\n",
    "\"\"\"\n",
    "class LabelCatcher(LearnerCallback):\n",
    "    last_labels = None\n",
    "\n",
    "    def __init__(self, learn:Learner):\n",
    "        super().__init__(learn)\n",
    "        \n",
    "    def on_batch_begin(self, last_input, last_target, train, **kwargs):\n",
    "        LabelCatcher.last_labels = last_target\n",
    "        return {'last_input': last_input, 'last_target': last_target} \n",
    "\n",
    "\n",
    "class XFaceNet(nn.Module):\n",
    "    def __init__(self, org_model, data, xface_product=ArcMarginProduct, m=0.5):\n",
    "        super().__init__()\n",
    "        self.org_model = org_model\n",
    "        self.feature_model = body_feature_model(org_model)\n",
    "        self.metric_fc = xface_product(512, data.c, m=m).cuda()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.feature_model(x)\n",
    "        x = self.metric_fc(x, LabelCatcher.last_labels)\n",
    "        return x\n",
    "\n",
    "\n",
    "def learner_ArcFace(train_data,train_num):\n",
    "    learn = cnn_learner(train_data, models.resnet18, metrics=accuracy)\n",
    "    learn.model = XFaceNet(learn.model, train_data, ArcMarginProduct, m=0.5)\n",
    "    learn.callback_fns.append(partial(LabelCatcher))\n",
    "    learn.fit(1)\n",
    "    learn.unfreeze()\n",
    "    learn.fit(train_num)\n",
    "    return learn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>14.825819</td>\n",
       "      <td>12.269404</td>\n",
       "      <td>0.059406</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='70' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      70.00% [70/100 03:08<01:20]\n",
       "    </div>\n",
       "    \n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>12.392128</td>\n",
       "      <td>9.734760</td>\n",
       "      <td>0.445545</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.689338</td>\n",
       "      <td>10.585092</td>\n",
       "      <td>0.519802</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>7.717578</td>\n",
       "      <td>9.960425</td>\n",
       "      <td>0.519802</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>6.270826</td>\n",
       "      <td>9.355492</td>\n",
       "      <td>0.574257</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.952044</td>\n",
       "      <td>8.778412</td>\n",
       "      <td>0.589109</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.379753</td>\n",
       "      <td>9.578675</td>\n",
       "      <td>0.564356</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.965181</td>\n",
       "      <td>10.300460</td>\n",
       "      <td>0.574257</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.274599</td>\n",
       "      <td>10.389733</td>\n",
       "      <td>0.579208</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.948407</td>\n",
       "      <td>7.895908</td>\n",
       "      <td>0.668317</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.995084</td>\n",
       "      <td>10.587428</td>\n",
       "      <td>0.589109</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.875304</td>\n",
       "      <td>9.544190</td>\n",
       "      <td>0.623762</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.411772</td>\n",
       "      <td>9.532446</td>\n",
       "      <td>0.633663</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.200714</td>\n",
       "      <td>8.362577</td>\n",
       "      <td>0.638614</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.173963</td>\n",
       "      <td>9.737440</td>\n",
       "      <td>0.613861</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.893226</td>\n",
       "      <td>10.370993</td>\n",
       "      <td>0.613861</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.919861</td>\n",
       "      <td>9.509678</td>\n",
       "      <td>0.608911</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.718685</td>\n",
       "      <td>10.006210</td>\n",
       "      <td>0.638614</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.713719</td>\n",
       "      <td>9.075040</td>\n",
       "      <td>0.648515</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.695676</td>\n",
       "      <td>9.791999</td>\n",
       "      <td>0.663366</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.568678</td>\n",
       "      <td>8.789868</td>\n",
       "      <td>0.678218</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.513438</td>\n",
       "      <td>9.028889</td>\n",
       "      <td>0.643564</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.413080</td>\n",
       "      <td>9.684200</td>\n",
       "      <td>0.653465</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.477218</td>\n",
       "      <td>10.545187</td>\n",
       "      <td>0.628713</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.392034</td>\n",
       "      <td>10.969214</td>\n",
       "      <td>0.638614</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.509666</td>\n",
       "      <td>11.327333</td>\n",
       "      <td>0.603960</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.468988</td>\n",
       "      <td>11.191627</td>\n",
       "      <td>0.628713</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.168882</td>\n",
       "      <td>10.965584</td>\n",
       "      <td>0.643564</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.195298</td>\n",
       "      <td>11.536408</td>\n",
       "      <td>0.608911</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.252671</td>\n",
       "      <td>10.056004</td>\n",
       "      <td>0.668317</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.151379</td>\n",
       "      <td>8.477584</td>\n",
       "      <td>0.717822</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.008060</td>\n",
       "      <td>10.106237</td>\n",
       "      <td>0.653465</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.827104</td>\n",
       "      <td>9.848584</td>\n",
       "      <td>0.653465</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.132957</td>\n",
       "      <td>9.819440</td>\n",
       "      <td>0.618812</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.960569</td>\n",
       "      <td>8.675403</td>\n",
       "      <td>0.678218</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.055217</td>\n",
       "      <td>8.798022</td>\n",
       "      <td>0.688119</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.016384</td>\n",
       "      <td>10.538027</td>\n",
       "      <td>0.638614</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.892525</td>\n",
       "      <td>10.548385</td>\n",
       "      <td>0.653465</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.793474</td>\n",
       "      <td>9.358892</td>\n",
       "      <td>0.683168</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.820865</td>\n",
       "      <td>10.737372</td>\n",
       "      <td>0.673267</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.878922</td>\n",
       "      <td>9.200876</td>\n",
       "      <td>0.678218</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.966850</td>\n",
       "      <td>8.318593</td>\n",
       "      <td>0.693069</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.124777</td>\n",
       "      <td>7.886252</td>\n",
       "      <td>0.712871</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.068156</td>\n",
       "      <td>10.449372</td>\n",
       "      <td>0.663366</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.756867</td>\n",
       "      <td>9.495132</td>\n",
       "      <td>0.638614</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.043107</td>\n",
       "      <td>11.638857</td>\n",
       "      <td>0.603960</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.036448</td>\n",
       "      <td>9.857986</td>\n",
       "      <td>0.643564</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.804885</td>\n",
       "      <td>11.823758</td>\n",
       "      <td>0.628713</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.826810</td>\n",
       "      <td>10.024440</td>\n",
       "      <td>0.633663</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.773102</td>\n",
       "      <td>11.705950</td>\n",
       "      <td>0.618812</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.723266</td>\n",
       "      <td>10.423355</td>\n",
       "      <td>0.643564</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.784492</td>\n",
       "      <td>10.872793</td>\n",
       "      <td>0.618812</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.739386</td>\n",
       "      <td>10.185004</td>\n",
       "      <td>0.653465</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.655721</td>\n",
       "      <td>12.366746</td>\n",
       "      <td>0.589109</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.795045</td>\n",
       "      <td>9.739106</td>\n",
       "      <td>0.683168</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.729441</td>\n",
       "      <td>9.306993</td>\n",
       "      <td>0.668317</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.884543</td>\n",
       "      <td>9.750657</td>\n",
       "      <td>0.688119</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.696074</td>\n",
       "      <td>10.795452</td>\n",
       "      <td>0.643564</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.499564</td>\n",
       "      <td>9.961529</td>\n",
       "      <td>0.668317</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.474029</td>\n",
       "      <td>9.755939</td>\n",
       "      <td>0.658416</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.497491</td>\n",
       "      <td>8.838830</td>\n",
       "      <td>0.722772</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.554356</td>\n",
       "      <td>9.070016</td>\n",
       "      <td>0.693069</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.544415</td>\n",
       "      <td>10.165518</td>\n",
       "      <td>0.663366</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.659901</td>\n",
       "      <td>11.077603</td>\n",
       "      <td>0.613861</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.623819</td>\n",
       "      <td>10.833219</td>\n",
       "      <td>0.638614</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.681335</td>\n",
       "      <td>10.822772</td>\n",
       "      <td>0.673267</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.634702</td>\n",
       "      <td>11.214868</td>\n",
       "      <td>0.658416</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.493291</td>\n",
       "      <td>10.062517</td>\n",
       "      <td>0.673267</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.424306</td>\n",
       "      <td>11.416246</td>\n",
       "      <td>0.653465</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.412216</td>\n",
       "      <td>12.320180</td>\n",
       "      <td>0.608911</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.440911</td>\n",
       "      <td>10.414713</td>\n",
       "      <td>0.638614</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='43' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/43 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11987/3440763984.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#訓練回数\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner_ArcFace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#learn.save('data_cnn_arcface_kimiaki')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#pred_class,pred_idx,outputs = learn.predict(img)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_11987/2549443002.py\u001b[0m in \u001b[0;36mlearner_ArcFace\u001b[0;34m(train_data, train_num)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, epochs, lr, wd, callbacks)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_fns\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextra_callback_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlistify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_opt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mFloats\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m->\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/fastai/basic_train.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(epochs, learn, callbacks, metrics)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpbar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcb_handler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/fastprogress/fastprogress.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/fastai/basic_data.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;34m\"Process and returns items from `DataLoader`.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/fastai/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#訓練回数\n",
    "train_num = 50\n",
    "learn = learner_ArcFace(data,train_num)\n",
    "#learn.save('data_cnn_arcface_kimiaki')\n",
    "#pred_class,pred_idx,outputs = learn.predict(img)  \n",
    "#print(pred_class) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴抽出（512次元）\n",
    "embs = get_embeddings(body_feature_model(learn.model,takubo=0), data.valid_dl)\n",
    "#t-sneで次元削除を行いグラフで可視化\n",
    "show_2D_tSNE(embs, [int(y) for y in data.valid_ds.y], title='SphereFace (t-SNE)')\n",
    "#t-sneで可視化（画像を貼り付けて）\n",
    "fig, ax = plt.subplots(figsize=(30,30))\n",
    "imscatter(path_l, embs, ax=ax, zoom=0.4,title=\"Simply resnet18\")\n",
    "#t-sneで可視化（3次元）\n",
    "show_3D_tSNE(embs, [int(y) for y in data.valid_ds.y], title='Simply resnet18 (t-SNE3D)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import image_clustering_2\n",
    "\"\"\"\n",
    "田久保が書いたクラスタリング(k-means++)のファイルをここで実行している。\n",
    "クラスタリングとARI（クラスタリングの評価）を出力する。\n",
    "embs　cnnから特徴抽出を行った値(512次元)\n",
    "n_clusters=3 クラスタ数←この数にクラス分けをしてくれる\n",
    "image_file_temp='img_%s.jpg'　どのファイルを扱うかをしてする（ここではjpgファイル）\n",
    "image_size=224　画像のサイズ\n",
    "t_SNE_OK=False　クラスタリングの前にt-sneを使用するかどうか\n",
    "\"\"\"\n",
    "s=image_clustering_2.Image_Clustering(embs, n_clusters=7, image_file_temp='img_%s.jpg', image_size=224,t_SNE_OK=False,image_path=image_path)\n",
    "s.main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grad_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grad_camについて\n",
    "\n",
    "\n",
    "\n",
    "# hook for the gradients \n",
    "def gradient_torch_hook(self, grad_input, grad_output):\n",
    "    return grad_input\n",
    "\n",
    "def image_from_tensor(imagetensor):\n",
    "    numpied = torch.squeeze(imagetensor)\n",
    "    numpied = np.moveaxis(numpied.cpu().numpy(), 0 , -1)\n",
    "    numpied = numpied - np.min(numpied)\n",
    "    numpied = numpied/np.max(numpied)\n",
    "    return numpied\n",
    "\n",
    "\n",
    "#returnを書く\n",
    "def Grad_cam(model,image,label,show_class_grad_cam,data,top50):\n",
    "    #どこの層でするか\n",
    "    #式で言うところのA\n",
    "    target_layer = model.org_model[0][7][1]\n",
    "    \n",
    "    #time\n",
    "    before = time.time()\n",
    "    \n",
    "    # hook for the feature maps\n",
    "    #レイヤー(target_layer)の出力を格納するだけのHookを作成する関数\n",
    "    fmap_hook = fastai.callbacks.hook_output(target_layer)\n",
    "    \n",
    "    gradient_hook = fastai.callbacks.Hook(target_layer, gradient_torch_hook, is_forward=False)\n",
    "    \n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "    \n",
    "   \n",
    "    \n",
    "    out = model.org_model(image)\n",
    "    #クラスに合わせてゼロを作成している\n",
    "    onehot = torch.zeros(data.c)\n",
    "\n",
    "    #今調べたいクラスを記入するどこをみているのか\n",
    "    class_grad_cam = show_class_grad_cam\n",
    "    onehot[class_grad_cam] = 1.0\n",
    "    out.backward(gradient=onehot.reshape(1, -1).cuda(), retain_graph=True)\n",
    "\n",
    "    gradients = next(iter(gradient_hook.stored))\n",
    "\n",
    "    gradient_linearization = gradients.cpu().numpy().sum((2, 3)).reshape(-1)\n",
    "    \n",
    "    #重みを平均化して、レイヤーのアウトプットに乗じる\n",
    "    #weights = np.mean(gradient_linearization, axis=(0, 1))\n",
    "    weights = np.mean(gradient_linearization)\n",
    "    label = np.argsort(weights)\n",
    "    \n",
    "    \n",
    "    fmaps = fmap_hook.stored.cpu().numpy()\n",
    "    fmaps = fmaps.reshape(512, 7, 7)\n",
    "\n",
    "    # relu on the heatmap\n",
    "    #einsumはアインシュタンンの縮約記法法\n",
    "    #gradient_linearization一元元\n",
    "    #fmaps三元元\n",
    "    #Σててる\n",
    "    heatmap = np.maximum(0, np.einsum('i, ijk',gradient_linearization, fmaps))\n",
    "\n",
    "    # we now upsample the heatmap so we can overlay it on our original image\n",
    "    upsampled = scipy.ndimage.zoom(heatmap, 32)\n",
    "    upsampled = (upsampled - np.min(upsampled))/(np.max(upsampled) - np.min(upsampled))\n",
    "    \n",
    "    return upsampled, gradient_linearization, time.time()-before\n",
    "    \n",
    "#画像を出力する。\n",
    "\"\"\"\n",
    "plt.imshow(image_from_tensor(image))\n",
    "plt.imshow(upsampled, alpha=.8)\n",
    "plt.gca().set_axis_off()\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def train_faster_gradcam(data,show_class_grad_cam,model,clusters=10,predict=False):\n",
    "     #ここにかく\n",
    "    \"\"\"\n",
    "    image=data.train_ds.x\n",
    "    label = []\n",
    "    for y in data.train_ds.y:\n",
    "        label.append(y)\n",
    "    \"\"\"\n",
    "    embs,y,x= get_embeddings(body_feature_model(model,takubo=0), data.train_dl,return_y=True)    \n",
    "    # Arcfaceを削除\n",
    "    model_embed = model.org_model[0][7][1]\n",
    "    \n",
    "    vector_anomaly = []\n",
    "    vector_normal = []\n",
    "    image_arr = []\n",
    "    image_nomaly_arr = []\n",
    "    label_arr = []\n",
    "    label_nomaly_arr = []\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        if show_class_grad_cam == int(y[i]):\n",
    "            vector_anomaly.append(embs[i])\n",
    "            image_arr.append(x[i])\n",
    "            label_arr.append(y[i])\n",
    "        else:\n",
    "            vector_normal.append(embs[i])\n",
    "            image_nomaly_arr.append(x[i])\n",
    "            label_nomaly_arr.append(y[i])\n",
    "            \n",
    "    \"\"\"\n",
    "    for i in range(len(image)):\n",
    "        if show_class_grad_cam == int(label[i]):\n",
    "            vector_anomaly.append(embs[i])\n",
    "            image_arr.append(image[i])\n",
    "            label_arr.append(label[i])\n",
    "    \"\"\"\n",
    "    \n",
    "    # k-means\n",
    "    kmeans = KMeans(n_clusters=clusters, random_state=0).fit(vector_anomaly)\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # channel database\n",
    "    channel_weight, channel_adress = [], []\n",
    "    temp_weight = np.zeros((clusters, 512))# 480=\"block_16_expand_relu\".output\n",
    "    print(\"Making Database...\")\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        # x_anomalyについて一個ずつ重みを加算していく\n",
    "        _, weight,_ = Grad_cam(model=model,image=image_arr[i],label=label_arr[i],show_class_grad_cam=show_class_grad_cam,data=data,top50=False)\n",
    "        #_, weight = Grad_cam(model=learn.model,image=img,label=label,show_class_grad_cam=4,data=data,top50=False)\n",
    "        temp_weight[labels[i]] += weight #要確認show_class_grad_cam\n",
    "        print(i+1,\"/\",len(labels))\n",
    "\n",
    "    for i in range(clusters):\n",
    "        number = np.where(labels == i, 1, 0) #クラスタ内の個数\n",
    "        average_weight = temp_weight[i] / np.sum(number) #重みの平均\n",
    "        weight_adress = np.argsort(average_weight)\n",
    "        channel_adress.append(weight_adress[-50:])\n",
    "        channel_weight.append(average_weight[weight_adress[-50:]])\n",
    "\n",
    "    return model_embed, kmeans, np.array(channel_weight), np.array(channel_adress), vector_normal\n",
    "\n",
    "\"\"\"\n",
    "x:image\n",
    "\n",
    "\"\"\"\n",
    "def predict_faster_gradcam(model, kmeans, channel_weight, channel_adress,image,label):\n",
    "    before = time.time()\n",
    "    \n",
    "    vector,channel_out,_ = get_embeddings_one(body_feature_model(model,takubo=0), image,label,return_y=True)\n",
    "    \n",
    "    target_layer = model.org_model[0][7][1]\n",
    "    fmap_hook = fastai.callbacks.hook_output(target_layer)\n",
    "    model.eval()\n",
    "    model.zero_grad()\n",
    "    \n",
    "    channel_out = model.org_model(image)\n",
    "    cluster_no = kmeans.predict(vector)\n",
    "    \n",
    "    fmaps = fmap_hook.stored.cpu().numpy()\n",
    "    fmaps = fmaps.reshape(512, 7, 7)\n",
    "    \n",
    "    heatmap = np.maximum(0, np.einsum('i, ijk',channel_weight[cluster_no][0], fmaps[channel_adress[cluster_no][0],:,:]))\n",
    "\n",
    "    # ヒートマップにして合成\n",
    "    upsampled = scipy.ndimage.zoom(heatmap, 32)\n",
    "    upsampled = (upsampled - np.min(upsampled))/(np.max(upsampled) - np.min(upsampled))\n",
    "    \"\"\"\n",
    "    # レイヤーのアウトプットに乗じる\n",
    "    cam = np.dot(channel_out[:,:,channel_adress[cluster_no][0]], channel_weight[cluster_no][0])\n",
    "\n",
    "    cam = cv2.resize(cam, (x.shape[1], x.shape[0]), cv2.INTER_LINEAR)\n",
    "    cam = np.maximum(cam, 0)\n",
    "    cam = cam / cam.max()\n",
    "    \n",
    "    jetcam = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)  # モノクロ画像に疑似的に色をつける\n",
    "    jetcam = cv2.cvtColor(jetcam, cv2.COLOR_BGR2RGB)  # 色をRGBに変換\n",
    "    jetcam = (np.float32(jetcam) + x*255 / 2)   # もとの画像に合成\n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "    return upsampled,time.time()-before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare grad and faster-grad\n",
    "#imageとlabelをつくらなきゃいけない\n",
    "#show_resultの変数のdataをわけてみるか\n",
    "#返り値に気をつけてコードを編集する\n",
    "\n",
    "def show_result(model, model_small, image,label,show_class_grad_cam, no, kmeans, channel_weight, channel_adress, top50=False):\n",
    "    original, result_grad, result_faster, time0, time1 = [], [], [], [], []\n",
    "    for i in range(5):\n",
    "        original.append(image[no[i]]) \n",
    "        img0, _, time_0 = Grad_cam(model=model,image=image[no[i]],label=label[no[i]],show_class_grad_cam=show_class_grad_cam,data=data,top50=False)\n",
    "        img1, time_1 = predict_faster_gradcam(model, kmeans, channel_weight, channel_adress,image[no[i]],label=label[no[i]])\n",
    "        result_grad.append(img0)\n",
    "        result_faster.append(img1)\n",
    "        time0.append(time_0)\n",
    "        time1.append(time_1)\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    for i in range(5):\n",
    "        plt.subplot(3,5,i+1)\n",
    "        plt.axis(\"off\")\n",
    "        if i == 0:\n",
    "            plt.title(\"original\")\n",
    "        plt.imshow(image_from_tensor(original[i]))\n",
    "    for i in range(5):\n",
    "        plt.subplot(3,5,i+6)\n",
    "        plt.axis(\"off\")\n",
    "        if i == 0:\n",
    "            time_ = int(np.mean(time1)*1000)\n",
    "            plt.title(\"Adapting-Grad-CAM \\n(%d msec)\" % time_)\n",
    "        plt.imshow(image_from_tensor(original[i]))\n",
    "        plt.imshow(result_faster[i], alpha=.8)\n",
    "    for i in range(5):\n",
    "        plt.subplot(3,5,i+11)\n",
    "        plt.axis(\"off\")\n",
    "        if i == 0:\n",
    "            time_ = int(np.mean(time0)*1000)\n",
    "            plt.title(\"Grad-CAM \\n(%d msec)\" % time_)\n",
    "        plt.imshow(image_from_tensor(original[i]))\n",
    "        plt.imshow(result_grad[i], alpha=.8)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#grad-cam見たいラべル\n",
    "\"\"\"\n",
    "対話型タスクにするためには\n",
    "show_class_grad_cam:grad-camに関してみたいラベルを記入する\n",
    "target_label:画像のラベルを記入\n",
    "\n",
    "通常のタスクにするためには\n",
    "show_class_grad_camとtarget_labelを合わせるべき\n",
    "\"\"\"\n",
    "show_class_grad_cam = 3\n",
    "target_label = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_grad, kmeans, channel_weight, channel_adress, vector_normal = train_faster_gradcam(data=data,show_class_grad_cam=show_class_grad_cam,model=learn.model, clusters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "labels = []\n",
    "\n",
    "for x,y in data.valid_dl:\n",
    "    for i in x:\n",
    "        imgs.append(i.data.to(\"cuda\").reshape(1, 3, 224, 224))\n",
    "    labels.append(y)\n",
    "labels = np.concatenate(labels)\n",
    "\n",
    "target_imgs=[]\n",
    "target_imgs_label = []\n",
    "for i in range(len(labels)):\n",
    "    if int(labels[i]) == target_label:\n",
    "        target_imgs.append(imgs[i])\n",
    "        target_imgs_label.append(labels[i])\n",
    "        \n",
    "        \n",
    "        \n",
    "len(target_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_result(model=learn.model, model_small=model_grad,image=target_imgs,label=target_imgs_label,show_class_grad_cam=show_class_grad_cam, no=[1,2,3,4,5],kmeans=kmeans, channel_weight=channel_weight, channel_adress=channel_adress, top50=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# リアルタイム"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "\n",
    "\n",
    "channel_weight = channel_weight\n",
    "channel_adress = channel_adress\n",
    "#vector_normalがvector_paに変更する。\n",
    "vector_normal = vector_normal\n",
    "kmeans = kmeans\n",
    "model = learn.model\n",
    "\n",
    "#x1とx2のcos類似度をはかる。そのスコアを出力する。\n",
    "def get_score_arc(vector_normal, test):\n",
    "    # cosine similarity\n",
    "    cos_similarity = cosine_similarity(test, vector_normal) # shape(len(test), len(train))\n",
    "\n",
    "    return np.max(cos_similarity)\n",
    "\n",
    "#x1とx2のcos類似度をはかる。\n",
    "def cosine_similarity(x1, x2):\n",
    "    #np.newaxisは計算するために次元をうまくやってくれるやつ\n",
    "    if x1.ndim == 1:\n",
    "        x1 = x1[np.newaxis]\n",
    "    if x2.ndim == 1:\n",
    "        x2 = x2[np.newaxis]\n",
    "    #ベクトルの大きさ（ノルム）を計算する手法\n",
    "    x1_norm = np.linalg.norm(x1, axis=1)\n",
    "    x2_norm = np.linalg.norm(x2, axis=1)\n",
    "    cosine_sim = np.dot(x1, x2.T)/(x1_norm*x2_norm+1e-10)\n",
    "    return cosine_sim\n",
    "\n",
    "#ここで推論をしている。ヒートマップを計算する関数\n",
    "#ここは後で書き直します\n",
    "def predict_faster_gradcam(channel, vector, img, kmeans, channel_weight, channel_adress):\n",
    "    channel_out = channel[0]\n",
    "    \n",
    "    # k-means and heat_map\n",
    "    cluster_no = kmeans.predict(vector)\n",
    "    fmaps = fmap_hook.stored.cpu().numpy()\n",
    "    fmaps = fmaps.reshape(512, 7, 7)\n",
    "    \n",
    "    heatmap = np.maximum(0, np.einsum('i, ijk',channel_weight[cluster_no][0], fmaps[channel_adress[cluster_no][0],:,:]))\n",
    "\n",
    "    # ヒートマップにして合成\n",
    "    upsampled = scipy.ndimage.zoom(heatmap, 32)\n",
    "    upsampled = (upsampled - np.min(upsampled))/(np.max(upsampled) - np.min(upsampled))\n",
    "    return upsampled\n",
    "\n",
    "def get_x_y_limit(heatmap, thresh):\n",
    "    #条件に応じた処理を行う\n",
    "    map_ = np.where(heatmap>thresh)\n",
    "    x_max = np.max(map_[1])\n",
    "    x_min = np.min(map_[1])\n",
    "    y_max = np.max(map_[0])\n",
    "    y_min = np.min(map_[0])\n",
    "\n",
    "    x_max = int(x_max)\n",
    "    x_min = int(x_min)\n",
    "    y_max = int(y_max)\n",
    "    y_min = int(y_min)\n",
    "    return x_min, y_min, x_max, y_max\n",
    "\n",
    "#bounding_boxを描く関数\n",
    "def bounding_box(img, x_min, y_min, x_max, y_max):\n",
    "    img = cv2.rectangle(img, (x_min, y_min), (x_max, y_max), (0, 255, 0), 5)\n",
    "    return img\n",
    "\n",
    "def main():\n",
    "    camera_width =  352\n",
    "    camera_height = 288\n",
    "    input_size = 96\n",
    "    hand_thresh = 0.25\n",
    "    OD_thresh = 0.8\n",
    "    fps = \"\"\n",
    "    message1 = \"Push [q] to quit.\"\n",
    "    message2 = \"Push [s] to change mode.\"\n",
    "    hand = \"\"\n",
    "    elapsedTime = 0\n",
    "    like_OD = False# like object detection\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    cap.set(cv2.CAP_PROP_FPS, 8)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH, camera_width)\n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, camera_height)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        t1 = time.time()\n",
    "\n",
    "        ret, image = cap.read()\n",
    "        image = image[:,32:320]\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (input_size, input_size))\n",
    "        #ここをかえる\n",
    "        channel_out = model.org_model(img)\n",
    "        test_vector = get_embeddings_one(body_feature_model(model,takubo=0), image=img, label=0, return_y=False)\n",
    "        score = get_score_arc(vector_pa, test_vector)\n",
    "\n",
    "        if score < hand_thresh: # hand is gu\n",
    "            hand = \"gu\"\n",
    "            color = (255, 0, 0)\n",
    "            heatmap = predict_faster_gradcam(channel_out, test_vector, image, kmeans, channel_weight, channel_adress)\n",
    "            if like_OD:\n",
    "                x_min, y_min, x_max, y_max = get_x_y_limit(heatmap, OD_thresh)\n",
    "                result = bounding_box(image, x_min, y_min, x_max, y_max)\n",
    "            else:\n",
    "                heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
    "                image = np.copy(cv2.addWeighted(heatmap, 0.5, image, 0.5, 2.2))\n",
    "\n",
    "        else:# hand is pa\n",
    "            hand = \"pa\"\n",
    "            color = (0, 0, 255)\n",
    "\n",
    "        # message\n",
    "        cv2.putText(image, \"{0} {1:.1f} Score\".format(hand, score),(camera_width - 290, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, message1, (camera_width - 285, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, message2, (camera_width - 285, 35), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        cv2.putText(image, fps, (camera_width - 175, 110), cv2.FONT_HERSHEY_SIMPLEX, 1, ( 255, 0 ,0), 1, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow(\"Result\", image)\n",
    "\n",
    "        # FPS\n",
    "        elapsedTime = time.time() - t1\n",
    "        fps = \"{:.0f} FPS\".format(1/elapsedTime)\n",
    "\n",
    "        # quit or change mode\n",
    "        key = cv2.waitKey(1)&0xFF\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "        if key == ord(\"s\"):\n",
    "            if like_OD == True:\n",
    "                like_OD = False\n",
    "            else:\n",
    "                like_OD = True\n",
    "\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "    cv2.imshow('video image', img)#'video image'はカメラのウィンドウの名前\n",
    "    key = cv2.waitKey(10)\n",
    "    if key == 27:  # ESCキーで終了\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
